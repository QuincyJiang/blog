title: 机器学习算法的演进
date: 2018/11/11 19:33:50
categories: 机器学习
comments: true
tags: [机器学习,Tensorflow]
---

## 人工智能、机器学习、深度学习三者的关系：
**人工智能/AI：**这是对高级计算智能的最宽泛的说法。
1956年，在达特茅斯人工智能大会上，该技术被描述为：“原则上，学习的每一个方面或任何其他智能特征都可以精确描述，并且一台机器可以模拟它。”。
AI 可以分为大致三个种类： 狭义AI、通用AI、超AI。
狭义AI 就类比与深蓝计算机以及AlphaGO，只在特定领域表现出色，而通用AI则彼此更高一层，成为与人类大脑具有相同本质的智慧体从而具有解决一系列问题的能力。
超AI则是目前人工智能科学家的终极梦想，此时的机器具备观察和感知的能力，具备超越人类的创造力。

**机器学习/ML：** 一种实现人工智能的方法机器学习最基本的做法，是使用算法来解析数据、从中学习，然后对真实世界中的事件做出决策和预测。与传统的为解决特定任务、硬编码的软件程序不同，机器学习是用大量的数据来“训练”，通过各种算法从数据中学习如何完成任务。举个简单的例子，当我们浏览网上商城时，经常会出现商品推荐的信息。这是商城根据你往期的购物记录和冗长的收藏清单，识别出这其中哪些是你真正感兴趣，并且愿意购买的产品。这样的决策模型，可以帮助商城为客户提供建议并鼓励产品消费。

机器学习直接来源于早期的人工智能领域，传统的算法包括[决策树、聚类、贝叶斯分类、支持向量机、EM、Adaboost等等](https://www.jiqizhixin.com/articles/a-tour-of-the-top-10-algorithms-for-machine-learning-newbies)。

从学习方法上来分，机器学习算法可以分为[监督学习（如分类问题）、无监督学习（如聚类问题）、半监督学习、集成学习、深度学习和强化学习](https://www.jianshu.com/p/d1c97a4d2e70)。

传统的机器学习算法在指纹识别、基于Haar的人脸检测、基于HoG特征的物体检测等领域的应用基本达到了商业化的要求或者特定场景的商业化水平，但每前进一步都异常艰难，直到深度学习算法的出现。

所谓深度学习的深度，指的是**神经网络的层数**，机器学习算法经过：

* 单层感知机：心理学家Rosenblatt提出，对复杂函数无能为力
* 多层感知机：多层感知机可以摆脱早期离散传输函数的束缚，使用`sigmoid`或`tanh`等连续函数模拟神经元对激励的响应，在训练算法上则使用*Werbos*发明的反向传播BP算法。这就是我们现在所说的**神经网络**，但多层感知机面临的致命问题是：**随着神经网络层数的加深：一是优化函数越来越容易陷入局部最优解，并且这个“陷阱”越来越偏离真正的全局最优。利用有限数据训练的深层网络，性能还不如较浅层网络。二：“梯度消失”现象更加严重**
* 深度学习：2006年，*Hinton*利用预训练方法缓解了局部最优解问题，将隐含层推动到了7层，神经网络真正意义上有了“深度”，由此揭开了深度学习的热潮，随后的`DBN`、`CNN`、`RNN`、`LSTM`等才逐渐出现。 这里的“深度”并没有固定的定义——在语音识别中4层网络就能够被认为是“较深的”，而在图像识别中20层以上的网络屡见不鲜。 为了克服梯度消失，`ReLU`、`maxout`等传输函数代替了`sigmoid`，形成了如今`DNN`的基本形式。单从结构上来说，全链接的多层感知机是没有任何区别的。

**深度学习/DL：** 如上所言，深度学习是机器学习算法演进过程中诞生的一种。深度学习本来并不是一种独立的学习方法，其本身也会用到有监督和无监督的学习方法来训练深度神经网络。但由于近几年该领域发展迅猛，一些特有的学习手段相继被提出（如[残差网络](https://my.oschina.net/u/876354/blog/1622896)），因此越来越多的人将其单独看作一种学习的方法。
## 深度神经网络的三个致命问题：
深度神经网络有三个致命问题：
### 非凸优化问题，即优化函数越来越容易陷入局部最优解；
线性回归，本质是一个多元一次函数的优化问题，设f(x,y)=x+y 
多层神经网络，本质是一个多元K次函数优化问题，设f(x,y)=xy 
在线性回归当中，从任意一个点出发搜索，最终必然是下降到全局最小值附近的。所以置0也无妨（这也是为什么我们往往解线性回归方程时初值为0）。 
而在多层神经网络中，从不同点出发，可能最终困在局部最小值。局部最小值是神经网络结构带来的挥之不去的阴影，随着隐层层数的增加，非凸的目标函数越来越复杂，局部最小值点成倍增长，利用有限数据训练的深层网络，性能还不如较浅层网络。。避免的方法一般是权值初始化。为了统一初始化方案，通常将输入缩放到[−1,1]，但是仍然无法保证能够达到全局最优，其实这也是科学家们一直在研究而未解决的问题。 
所以，从本质上来看，深度结构带来的非凸优化仍然不能解决（包括现在的各类深度学习算法和其他非凸优化问题都是如此），这限制着深度结构的发展。

### （Gradient Vanish）梯度消失问题；
梯度爆炸和梯度消失问题都是因为网络太深，网络权值更新不稳定造成的，本质上是因为梯度反向传播中的连乘效应。
对于更普遍的梯度消失问题，可以考虑用ReLU激活函数取代sigmoid激活函数。另外，LSTM的结构设计也可以改善RNN中的梯度消失问题

### 过拟合问题
过拟合，庞大的结构和参数使得，尽管训练error降的很低，但是test error却高的离谱。 
当出现梯度消失时，上层神经元的梯度变化非常小，几乎相当于把原始输入信息，没有经过任何非线性变换，或者错误变换推到高层去，使得高层解离特征压力太大。 
如果特征无法解离，强制性的误差监督训练就会使得模型对输入数据直接做拟合，就会出现训练方差大的问题


## 深度学习的常见模型：

[这张图基本囊括了目前常见的主流神经网络模型](https://towardsdatascience.com/the-mostly-complete-chart-of-neural-networks-explained-3fb6f2367464)
![](/media/15419350474031.jpg)



